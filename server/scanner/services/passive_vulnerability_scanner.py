# backend/scanner/services/passive_vulnerability_scanner.py

import requests
import logging
import re
import time
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
from urllib.robotparser import RobotFileParser

logger = logging.getLogger(__name__)

class PassiveVulnerabilityScanner:
    """
    Passive vulnerability scanner that performs only non-intrusive analysis.
    Safe to run on any website without requiring authorization.
    """
    
    def __init__(self, url):
        self.url = url
        self.base_url = self._get_base_url(url)
        self.domain = urlparse(url).netloc
        
        self.headers = {
            'User-Agent': 'Site-Analyser Passive Scanner/1.0 (Passive Mode Only)'
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # Passive scanning limits
        self.max_pages_to_scan = 3  # Very limited for passive scanning
        self.request_delay = 1.0  # Be respectful
        self.max_requests = 10  # Very limited
        
        self.scanned_urls = set()
        self.requests_made = 0
        self.last_request_time = 0
    
    def _get_base_url(self, url):
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}"
    
    def _rate_limit(self):
        """Implement respectful rate limiting"""
        elapsed = time.time() - self.last_request_time
        if elapsed < self.request_delay:
            time.sleep(self.request_delay - elapsed)
        self.last_request_time = time.time()
        
        self.requests_made += 1
        if self.requests_made > self.max_requests:
            raise Exception(f"Maximum request limit ({self.max_requests}) exceeded for passive scanning")
    
    def _make_request(self, method, url, **kwargs):
        """Make a request with rate limiting"""
        self._rate_limit()
        
        try:
            kwargs.setdefault('timeout', 10)
            kwargs.setdefault('allow_redirects', True)
            
            # Add passive scanning headers
            if 'headers' not in kwargs:
                kwargs['headers'] = {}
            kwargs['headers'].update({
                'X-Scanner-Type': 'passive',
                'X-Scanner-Purpose': 'security-assessment-passive'
            })
            
            response = self.session.request(method, url, **kwargs)
            return response
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Passive request failed for {url}: {str(e)}")
            return None
    
    def scan(self):
        """Run passive vulnerability scanning"""
        findings = []
        
        logger.info(f"Starting passive vulnerability scan for {self.url}")
        
        try:
            # Add passive scan disclaimer
            findings.append(self._generate_passive_disclaimer())
            
            # Check robots.txt for any security-related restrictions
            findings.extend(self._check_robots_txt())
            
            # Test for sensitive file exposure (passive only)
            findings.extend(self._test_sensitive_file_exposure_passive())
            
            # Check for information disclosure in HTML
            findings.extend(self._test_information_disclosure_passive())
            
            # Check form security (passive analysis only)
            findings.extend(self._check_form_security_passive())
            
            # Check for common security misconfigurations (passive)
            findings.extend(self._check_security_misconfigurations_passive())
            
            # Add passive scan summary
            findings.append(self._generate_passive_summary())
            
        except Exception as e:
            logger.exception(f"Error in passive vulnerability scan: {str(e)}")
            findings.append({
                'name': 'Passive Vulnerability Scan Error',
                'description': f'Error during passive vulnerability scanning: {str(e)}',
                'severity': 'info',
                'details': {'error': str(e)}
            })
        
        return findings
    
    def _generate_passive_disclaimer(self):
        """Generate disclaimer for passive scan results"""
        return {
            'name': 'Passive Scan Notice',
            'description': 'This is a passive security scan that only analyzes publicly available information.',
            'severity': 'info',
            'details': {
                'scan_type': 'passive',
                'notice': 'This scan performs only non-intrusive analysis and is safe to run on any website.',
                'limitations': [
                    'Cannot detect vulnerabilities that require active testing',
                    'May not find all security issues',
                    'Results should be verified manually',
                    'Some findings may be false positives'
                ],
                'recommendation': 'For comprehensive security testing, consider authorized active scanning.'
            }
        }
    
    def _generate_passive_summary(self):
        """Generate passive scan summary"""
        return {
            'name': 'Passive Scan Summary',
            'description': f'Passive vulnerability scan completed for {self.domain}',
            'severity': 'info',
            'details': {
                'scan_type': 'passive',
                'pages_analyzed': len(self.scanned_urls),
                'requests_made': self.requests_made,
                'domain': self.domain,
                'passive_features': [
                    'Respectful rate limiting',
                    'No intrusive testing',
                    'Public information analysis only',
                    'Safe for any website'
                ]
            }
        }
    
    def _check_robots_txt(self):
        """Check robots.txt for security-related information"""
        findings = []
        
        try:
            robots_url = urljoin(self.base_url, '/robots.txt')
            response = self._make_request('GET', robots_url)
            
            if response and response.status_code == 200:
                robots_content = response.text
                
                # Look for potentially sensitive paths mentioned in robots.txt
                sensitive_patterns = [
                    r'/admin', r'/administrator', r'/wp-admin', r'/wp-login',
                    r'/api', r'/database', r'/config', r'/backup',
                    r'/private', r'/internal', r'/secret', r'/hidden'
                ]
                
                found_paths = []
                for pattern in sensitive_patterns:
                    matches = re.findall(pattern, robots_content, re.IGNORECASE)
                    if matches:
                        found_paths.extend(matches)
                
                if found_paths:
                    findings.append({
                        'name': 'Sensitive Paths in robots.txt',
                        'description': f'robots.txt mentions {len(found_paths)} potentially sensitive paths',
                        'severity': 'low',
                        'details': {
                            'robots_url': robots_url,
                            'sensitive_paths': list(set(found_paths))[:10],  # Limit to 10
                            'impact': 'robots.txt may inadvertently reveal sensitive directory structures',
                            'recommendation': 'Review robots.txt to ensure it doesn\'t reveal sensitive information'
                        }
                    })
                
        except Exception as e:
            logger.debug(f"Could not check robots.txt: {str(e)}")
        
        return findings
    
    def _test_sensitive_file_exposure_passive(self):
        """Test for sensitive file exposure using only HEAD requests"""
        findings = []
        
        # Very limited file list for passive scanning
        sensitive_files = [
            '/.env', '/config.php', '/wp-config.php.bak',
            '/.git/config', '/package.json', '/.DS_Store'
        ]
        
        for file_path in sensitive_files[:5]:  # Limit files tested
            file_url = urljoin(self.base_url, file_path)
            response = self._make_request('HEAD', file_url)  # Use HEAD to be non-intrusive
            
            if response and response.status_code == 200:
                findings.append({
                    'name': 'Potentially Sensitive File Accessible',
                    'description': f'File may be publicly accessible: {file_path}',
                    'severity': 'medium',
                    'details': {
                        'file_url': file_url,
                        'file_path': file_path,
                        'content_type': response.headers.get('Content-Type', 'Unknown'),
                        'detection_method': 'passive_head_request',
                        'impact': f'File {file_path} may contain sensitive information',
                        'recommendation': f'Verify if {file_path} should be publicly accessible'
                    }
                })
        
        return findings
    
    def _test_information_disclosure_passive(self):
        """Test for information disclosure through passive analysis"""
        findings = []
        
        response = self._make_request('GET', self.url)
        if not response:
            return findings
        
        self.scanned_urls.add(self.url)
        
        # Check for verbose error messages in HTML comments
        soup = BeautifulSoup(response.text, 'html.parser')
        comments = soup.find_all(string=lambda text: isinstance(text, str) and '<!--' in text)
        
        sensitive_info = []
        for comment in comments[:5]:  # Limit comments analyzed
            comment_text = comment.strip()
            
            # Look for potentially sensitive information
            sensitive_patterns = [
                (r'version\s*[:=]\s*[\d.]+', 'Version Information'),
                (r'todo|fixme|bug', 'Development Comments'),
                (r'password|secret|key|token', 'Credential References'),
                (r'database|db|sql', 'Database References'),
                (r'api[_\s]key|apikey', 'API Key References')
            ]
            
            for pattern, category in sensitive_patterns:
                if re.search(pattern, comment_text, re.IGNORECASE):
                    sensitive_info.append({
                        'category': category,
                        'comment_preview': comment_text[:100] + ('...' if len(comment_text) > 100 else '')
                    })
                    break
        
        if sensitive_info:
            findings.append({
                'name': 'Information Disclosure in HTML Comments',
                'description': f'Found {len(sensitive_info)} HTML comments with potentially sensitive information',
                'severity': 'low',
                'details': {
                    'sensitive_comments': sensitive_info,
                    'page_url': self.url,
                    'impact': 'HTML comments may reveal sensitive development or configuration information',
                    'recommendation': 'Remove sensitive information from HTML comments in production'
                }
            })
        
        # Check for exposed version information in meta tags
        version_findings = self._check_version_disclosure_passive(soup)
        findings.extend(version_findings)
        
        return findings
    
    def _check_version_disclosure_passive(self, soup):
        """Check for version disclosure in meta tags"""
        findings = []
        
        # Check generator meta tags
        generators = soup.find_all('meta', attrs={'name': re.compile(r'generator', re.I)})
        
        for generator in generators:
            content = generator.get('content', '')
            if content:
                # Look for version numbers in generator tags
                version_match = re.search(r'(\d+\.\d+(?:\.\d+)?)', content)
                if version_match:
                    findings.append({
                        'name': 'Software Version Disclosure',
                        'description': f'Software version disclosed in meta generator tag: {content}',
                        'severity': 'low',
                        'details': {
                            'meta_content': content,
                            'version': version_match.group(1),
                            'page_url': self.url,
                            'impact': 'Version information helps attackers target known vulnerabilities',
                            'recommendation': 'Remove or obfuscate version information in meta tags'
                        }
                    })
        
        return findings
    
    def _check_form_security_passive(self):
        """Passive form security analysis"""
        findings = []
        
        response = self._make_request('GET', self.url)
        if not response:
            return findings
        
        soup = BeautifulSoup(response.text, 'html.parser')
        forms = soup.find_all('form')
        
        for i, form in enumerate(forms[:3]):  # Limit forms analyzed
            # Check for forms submitting over HTTP
            action = form.get('action', '')
            method = form.get('method', 'GET').upper()
            
            if action.startswith('http:') or (not action.startswith('https:') and urlparse(self.url).scheme == 'http'):
                findings.append({
                    'name': 'Form Submitting Over HTTP',
                    'description': 'Form may submit sensitive data over unencrypted connection',
                    'severity': 'medium',
                    'details': {
                        'form_action': action or 'same page',
                        'form_method': method,
                        'form_number': i + 1,
                        'page_url': self.url,
                        'impact': 'Form data could be intercepted during transmission',
                        'recommendation': 'Use HTTPS for all form submissions'
                    }
                })
            
            # Check for missing CSRF protection (passive detection)
            has_csrf_token = any(
                'csrf' in input_field.get('name', '').lower() or 
                'token' in input_field.get('name', '').lower() or
                'authenticity' in input_field.get('name', '').lower()
                for input_field in form.find_all('input', type='hidden')
            )
            
            # Only flag POST forms without CSRF tokens
            if method == 'POST' and not has_csrf_token:
                findings.append({
                    'name': 'Form May Lack CSRF Protection',
                    'description': 'POST form appears to lack CSRF protection tokens',
                    'severity': 'medium',
                    'details': {
                        'form_method': method,
                        'form_number': i + 1,
                        'page_url': self.url,
                        'impact': 'Form may be vulnerable to cross-site request forgery attacks',
                        'recommendation': 'Implement CSRF tokens for state-changing forms'
                    }
                })
        
        return findings
    
    def _check_security_misconfigurations_passive(self):
        """Check for common security misconfigurations through passive analysis"""
        findings = []
        
        response = self._make_request('GET', self.url)
        if not response:
            return findings
        
        # Check for directory listing
        if 'Index of /' in response.text or '<title>Index of' in response.text:
            findings.append({
                'name': 'Potential Directory Listing',
                'description': 'Page may be showing directory listing',
                'severity': 'medium',
                'details': {
                    'page_url': self.url,
                    'impact': 'Directory listings can expose sensitive files and directory structure',
                    'recommendation': 'Disable directory listing on web server'
                }
            })
        
        # Check for default server pages
        default_page_indicators = [
            'Apache2 Ubuntu Default Page',
            'IIS Windows Server',
            'nginx default page',
            'Welcome to nginx',
            'Apache HTTP Server Test Page'
        ]
        
        for indicator in default_page_indicators:
            if indicator in response.text:
                findings.append({
                    'name': 'Default Server Page Detected',
                    'description': f'Page appears to be a default server installation page',
                    'severity': 'low',
                    'details': {
                        'page_indicator': indicator,
                        'page_url': self.url,
                        'impact': 'Default pages may reveal server information and suggest incomplete configuration',
                        'recommendation': 'Replace default pages with proper content or remove them'
                    }
                })
                break
        
        return findings