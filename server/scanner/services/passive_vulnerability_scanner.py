import logging
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

logger = logging.getLogger(__name__)

class PassiveVulnerabilityScanner:
    def __init__(self, url, max_pages_to_scan=10):
        self.url = url
        self.max_pages_to_scan = max_pages_to_scan
        self.scanned_urls = set()
        self.headers = {
            'User-Agent': 'Site-Analyser Security Scanner/1.0'
        }

    def scan(self):
        findings = []
        urls_to_scan = [self.url]

        logger.info(f"Starting passive vulnerability scan for {self.url}")

        try:
            findings.append(self._generate_passive_disclaimer())

            while urls_to_scan and len(self.scanned_urls) < self.max_pages_to_scan:
                page_url = urls_to_scan.pop(0)
                if page_url in self.scanned_urls:
                    continue

                response = self._make_request("GET", page_url)
                if not response:
                    continue

                self.scanned_urls.add(page_url)
                soup = BeautifulSoup(response.text, 'html.parser')

                findings.extend(self._check_form_security_passive(soup, page_url))
                findings.extend(self._check_security_misconfigurations_passive(response, page_url))
                findings.extend(self._test_information_disclosure_passive(soup, page_url))

                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if not href.startswith('#'):
                        full_url = urljoin(page_url, href)
                        if self._is_internal_url(full_url) and full_url not in self.scanned_urls:
                            urls_to_scan.append(full_url)

            findings.extend(self._check_robots_txt())
            findings.extend(self._test_sensitive_file_exposure_passive())

            findings.append(self._generate_passive_summary())

        except Exception as e:
            logger.exception(f"Error in passive vulnerability scan: {str(e)}")
            findings.append({
                'name': 'Passive Vulnerability Scan Error',
                'description': f'Error during passive vulnerability scanning: {str(e)}',
                'severity': 'info',
                'details': {'error': str(e)}
            })

        return findings

    def _make_request(self, method, url):
        try:
            response = requests.request(method, url, headers=self.headers, timeout=10)
            return response if response.status_code == 200 else None
        except requests.RequestException as e:
            logger.warning(f"Request failed for {url}: {e}")
            return None

    def _is_internal_url(self, url):
        return urlparse(url).netloc == urlparse(self.url).netloc

    def _generate_passive_disclaimer(self):
        return {
            'name': 'Passive Scan Notice',
            'description': 'This scan is passive and non-intrusive.',
            'severity': 'info',
            'details': {
                'impact': 'Does not affect target system behavior.',
                'recommendation': 'For deeper analysis, consider active scanning.'
            }
        }

    def _generate_passive_summary(self):
        return {
            'name': 'Passive Scan Summary',
            'description': f'Scanned {len(self.scanned_urls)} pages passively.',
            'severity': 'info',
            'details': {
                'total_pages': len(self.scanned_urls),
                'scanned_urls': list(self.scanned_urls)
            }
        }

    def _check_form_security_passive(self, soup, page_url):
        findings = []
        forms = soup.find_all('form')
        for form in forms:
            if form.get('method', '').lower() != 'post':
                findings.append({
                    'name': 'Insecure Form Method',
                    'description': 'A form uses a non-POST method.',
                    'severity': 'low',
                    'details': {
                        'page_url': page_url,
                        'form_method': form.get('method'),
                        'recommendation': 'Use POST for form submissions to enhance data privacy.'
                    }
                })
            if not form.get('action', '').startswith('https'):
                findings.append({
                    'name': 'Form Action Not Secure',
                    'description': 'A form action does not use HTTPS.',
                    'severity': 'low',
                    'details': {
                        'page_url': page_url,
                        'form_action': form.get('action'),
                        'recommendation': 'Ensure form actions submit over HTTPS.'
                    }
                })
        return findings

    def _check_security_misconfigurations_passive(self, response, page_url):
        findings = []
        headers = response.headers

        if 'X-Content-Type-Options' not in headers:
            findings.append({
                'name': 'Missing X-Content-Type-Options Header',
                'description': 'The response is missing the X-Content-Type-Options header.',
                'severity': 'low',
                'details': {
                    'page_url': page_url,
                    'recommendation': 'Add the header with value "nosniff" to prevent MIME type sniffing.'
                }
            })

        if 'Content-Security-Policy' not in headers:
            findings.append({
                'name': 'Missing Content-Security-Policy Header',
                'description': 'The response lacks a Content-Security-Policy header.',
                'severity': 'low',
                'details': {
                    'page_url': page_url,
                    'recommendation': 'Define a Content Security Policy to mitigate XSS and other attacks.'
                }
            })

        return findings

    def _test_information_disclosure_passive(self, soup, page_url):
        findings = []
        comments = soup.find_all(string=lambda text: isinstance(text, str) and 'password' in text.lower())
        if comments:
            findings.append({
                'name': 'Potential Information Disclosure in Comments',
                'description': 'HTML comments may contain sensitive information.',
                'severity': 'low',
                'details': {
                    'page_url': page_url,
                    'snippet': comments[0][:100],
                    'recommendation': 'Remove sensitive data from comments in production code.'
                }
            })
        return findings

    def _check_robots_txt(self):
        findings = []
        robots_url = urljoin(self.url, '/robots.txt')
        try:
            response = requests.get(robots_url, headers=self.headers, timeout=5)
            if response.status_code == 200 and 'Disallow:' in response.text:
                findings.append({
                    'name': 'robots.txt Found',
                    'description': 'The site provides a robots.txt file.',
                    'severity': 'info',
                    'details': {
                        'robots_url': robots_url,
                        'sample': response.text[:500]
                    }
                })
        except requests.RequestException:
            pass
        return findings

    def _test_sensitive_file_exposure_passive(self):
        findings = []
        sensitive_paths = ['/admin', '/.env', '/.git']
        for path in sensitive_paths:
            full_url = urljoin(self.url, path)
            try:
                response = requests.get(full_url, headers=self.headers, timeout=5)
                if response.status_code == 200:
                    findings.append({
                        'name': f'Accessible Sensitive Path: {path}',
                        'description': f'{path} is publicly accessible.',
                        'severity': 'medium',
                        'details': {
                            'page_url': full_url,
                            'recommendation': 'Restrict access to sensitive paths.'
                        }
                    })
            except requests.RequestException:
                continue
        return findings
