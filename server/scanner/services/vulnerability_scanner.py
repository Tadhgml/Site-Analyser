# backend/scanner/services/vulnerability_scanner.py

import requests
import logging
import re
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import hashlib

logger = logging.getLogger(__name__)

class VulnerabilityScanner:
    """Scanner for common web vulnerabilities with enhanced detail"""
    
    def __init__(self, url):
        self.url = url
        self.base_url = self._get_base_url(url)
        self.headers = {
            'User-Agent': 'Site-Analyser Security Scanner/1.0'
        }
        self.scanned_urls = set()
        self.max_pages_to_scan = 10  # Limit the number of pages to scan to avoid excessive scanning
    
    def _get_base_url(self, url):
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}"
    
    def scan(self):
        """Scan the target URL for common vulnerabilities with enhanced detail"""
        findings = []
        pages_to_scan = [self.url]
        
        try:
            # Scan the main URL and a limited number of linked pages
            for page_url in pages_to_scan:
                if page_url in self.scanned_urls or len(self.scanned_urls) >= self.max_pages_to_scan:
                    continue
                
                self.scanned_urls.add(page_url)
                
                # Make request to target URL
                response = requests.get(page_url, headers=self.headers, timeout=10)
                
                # Store the response content
                content = response.text
                
                # Parse HTML
                soup = BeautifulSoup(content, 'html.parser')
                
                # Check for various vulnerabilities with page-specific information
                findings.extend(self._check_sensitive_files(page_url))
                findings.extend(self._check_exposed_version_info(content, page_url))
                findings.extend(self._check_form_security(soup, page_url))
                findings.extend(self._check_content_disclosure(content, page_url))
                findings.extend(self._check_javascript_libraries(soup, page_url))
                
                # Find additional pages to scan
                if len(pages_to_scan) < self.max_pages_to_scan:
                    for link in soup.find_all('a', href=True):
                        href = link['href']
                        if href.startswith('/') or href.startswith(self.base_url):
                            full_url = urljoin(self.base_url, href)
                            if full_url not in pages_to_scan and full_url not in self.scanned_urls:
                                pages_to_scan.append(full_url)
            
        except requests.exceptions.RequestException as e:
            logger.error(f"Error scanning vulnerabilities for {self.url}: {str(e)}")
            findings.append({
                'name': 'Connection Error',
                'description': f'Failed to connect to {self.url}: {str(e)}',
                'severity': 'info',
                'details': {'error': str(e)}
            })
        
        return findings
    
    def _check_sensitive_files(self, page_url):
        findings = []
        sensitive_files = [
            '/robots.txt',
            '/.git/',
            '/.env',
            '/wp-config.php',
            '/phpinfo.php',
            '/admin/',
            '/backup/',
            '/config.php',
            '/.htaccess',
            '/server-status',
            '/wp-json/',
            '/.DS_Store',
            '/package.json',
            '/package-lock.json',
            '/yarn.lock',
            '/composer.json',
            '/composer.lock',
            '/.npmrc',
            '/.github/',
        ]
        
        for file_path in sensitive_files:
            file_url = urljoin(self.base_url, file_path)
            try:
                response = requests.head(file_url, headers=self.headers, timeout=5)
                
                if response.status_code == 200:
                    # Try to get the content to provide more detail
                    content_preview = None
                    try:
                        content_response = requests.get(file_url, headers=self.headers, timeout=5)
                        content_preview = content_response.text[:500] if content_response.text else None
                    except:
                        pass
                    
                    findings.append({
                        'name': 'Sensitive File Exposed',
                        'description': f'The file or directory {file_path} is accessible, which may expose sensitive information.',
                        'severity': 'high',
                        'details': {
                            'url': file_url,
                            'status_code': response.status_code,
                            'content_type': response.headers.get('Content-Type', 'Unknown'),
                            'content_preview': content_preview,
                            'discovered_from': page_url,
                            'impact': 'Sensitive files may contain configuration data, passwords, or other information that could be used by attackers.',
                            'recommendation': f'Restrict access to {file_path} or remove it if not needed.'
                        }
                    })
            except requests.exceptions.RequestException:
                # Skip if the request fails
                pass
        
        return findings
    
    def _check_exposed_version_info(self, content, page_url):
        findings = []
        
        # Check for version information in HTML comments and meta data
        version_patterns = [
            (r'(?i)wordpress\s+v?(\d+\.\d+\.\d+)', 'WordPress'),
            (r'(?i)joomla\s+v?(\d+\.\d+\.\d+)', 'Joomla'),
            (r'(?i)drupal\s+v?(\d+\.\d+\.\d+)', 'Drupal'),
            (r'(?i)php\s+v?(\d+\.\d+\.\d+)', 'PHP'),
            (r'(?i)jquery\s+v?(\d+\.\d+\.\d+)', 'jQuery'),
            (r'(?i)angular\s+v?(\d+\.\d+\.\d+)', 'Angular'),
            (r'(?i)react\s+v?(\d+\.\d+\.\d+)', 'React'),
            (r'(?i)bootstrap\s+v?(\d+\.\d+\.\d+)', 'Bootstrap'),
            (r'(?i)node\.?js\s+v?(\d+\.\d+\.\d+)', 'Node.js'),
        ]
        
        # Also check HTML meta generator tags
        soup = BeautifulSoup(content, 'html.parser')
        generator_tags = soup.find_all('meta', attrs={'name': 'generator'})
        
        for tag in generator_tags:
            content_attr = tag.get('content', '')
            if content_attr:
                for pattern, software_name in version_patterns:
                    matches = re.findall(pattern, content_attr)
                    for match in matches:
                        findings.append({
                            'name': 'Software Version Exposed',
                            'description': f'The website is exposing software version information: {software_name} {match} in meta generator tag',
                            'severity': 'medium',
                            'details': {
                                'page_url': page_url,
                                'software': software_name,
                                'version': match,
                                'location': 'Meta generator tag',
                                'html_context': str(tag),
                                'impact': 'Attackers can use version information to target known vulnerabilities.',
                                'recommendation': 'Remove version information from meta tags and HTML comments.'
                            }
                        })
        
        # Check for version info in HTML comments and content
        for pattern, software_name in version_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                context = self._get_context_for_match(content, match, software_name)
                findings.append({
                    'name': 'Software Version Exposed',
                    'description': f'The website is exposing software version information: {software_name} {match}',
                    'severity': 'medium',
                    'details': {
                        'page_url': page_url,
                        'software': software_name,
                        'version': match,
                        'context': context,
                        'impact': 'Attackers can use version information to target known vulnerabilities.',
                        'recommendation': 'Remove version information from public-facing pages.'
                    }
                })
        
        return findings
    
    def _get_context_for_match(self, content, match, software_name):
        """Get surrounding context for a matched string in content"""
        try:
            pattern = f"{software_name}[^\\n]*{match}"
            match_obj = re.search(pattern, content, re.IGNORECASE)
            if match_obj:
                start_pos = max(0, match_obj.start() - 50)
                end_pos = min(len(content), match_obj.end() + 50)
                return f"...{content[start_pos:end_pos]}..."
            return "Context not available"
        except:
            return "Context not available"
    
    def _check_form_security(self, soup, page_url):
        findings = []
        
        # Check forms for proper security measures
        forms = soup.find_all('form')
        for i, form in enumerate(forms):
            # Create a unique identifier for this form
            form_id = form.get('id', f'form-{i+1}')
            form_html = str(form)
            form_hash = hashlib.md5(form_html.encode()).hexdigest()[:8]
            
            # Check if form is using HTTPS
            action = form.get('action', '')
            if action.startswith('http:'):
                findings.append({
                    'name': 'Insecure Form Action',
                    'description': 'A form on the website is submitting data over HTTP, which is insecure.',
                    'severity': 'high',
                    'details': {
                        'page_url': page_url,
                        'form_id': form_id,
                        'form_hash': form_hash,
                        'form_action': action,
                        'form_html': form_html[:300] + ('...' if len(form_html) > 300 else ''),
                        'location': f"Form #{i+1} on page",
                        'impact': 'Data submitted through this form can be intercepted by attackers.',
                        'recommendation': 'Update the form to use HTTPS for data submission.'
                    }
                })
            
            # Check for CSRF protection
            has_csrf_token = False
            inputs = form.find_all('input', type='hidden')
            for input_field in inputs:
                name = input_field.get('name', '').lower()
                if 'csrf' in name or 'token' in name or '_token' in name:
                    has_csrf_token = True
                    break
            
            if not has_csrf_token:
                findings.append({
                    'name': 'Missing CSRF Protection',
                    'description': 'A form on the website appears to be missing CSRF protection.',
                    'severity': 'medium',
                    'details': {
                        'page_url': page_url,
                        'form_id': form_id,
                        'form_hash': form_hash,
                        'form_action': form.get('action', 'N/A'),
                        'form_method': form.get('method', 'GET'),
                        'form_html': form_html[:300] + ('...' if len(form_html) > 300 else ''),
                        'location': f"Form #{i+1} on page",
                        'impact': 'Without CSRF protection, attackers can trick users into submitting forms without their knowledge.',
                        'recommendation': 'Implement CSRF tokens for all forms to prevent cross-site request forgery attacks.'
                    }
                })
                
            # Check for autocomplete on sensitive fields
            sensitive_inputs = form.find_all('input', attrs={'type': ['password', 'text']})
            for input_field in sensitive_inputs:
                field_name = input_field.get('name', '').lower()
                autocomplete = input_field.get('autocomplete', '')
                
                # Check if this is a sensitive field
                is_sensitive = any(keyword in field_name for keyword in ['pass', 'pwd', 'ssn', 'card', 'cvv', 'secret'])
                
                if is_sensitive and autocomplete != 'off' and 'password' in input_field.get('type', ''):
                    findings.append({
                        'name': 'Autocomplete Not Disabled on Sensitive Field',
                        'description': 'A sensitive form field does not have autocomplete disabled.',
                        'severity': 'low',
                        'details': {
                            'page_url': page_url,
                            'form_id': form_id,
                            'form_hash': form_hash,
                            'field_name': input_field.get('name', 'N/A'),
                            'field_type': input_field.get('type', 'N/A'),
                            'field_html': str(input_field),
                            'location': f"Form #{i+1} on page",
                            'impact': 'Browsers may save sensitive data, potentially exposing it to other users of the same device.',
                            'recommendation': 'Add autocomplete="off" to sensitive form fields or use autocomplete="new-password" for password fields.'
                        }
                    })
        
        return findings
    
    def _check_content_disclosure(self, content, page_url):
        findings = []
        
        # Check for sensitive information in page content
        sensitive_patterns = [
            (r'(?i)password.*[\'"][^\'"]{8,}[\'"]', 'Password'),
            (r'(?i)api[-_]?key.*[\'"][^\'"]{8,}[\'"]', 'API Key'),
            (r'(?i)secret[-_]?key.*[\'"][^\'"]{8,}[\'"]', 'Secret Key'),
            (r'(?i)access[-_]?token.*[\'"][^\'"]{8,}[\'"]', 'Access Token'),
            (r'(?i)\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b', 'IP Address'),
            (r'(?i)error|exception|stack trace|debug', 'Error/Debug Information'),
            (r'(?i)BEGIN (?:RSA|DSA|EC|OPENSSH) PRIVATE KEY', 'Private Key'),
            (r'(?i)-----BEGIN CERTIFICATE-----', 'Certificate'),
            (r'(?i)(?:SELECT|INSERT|UPDATE|DELETE|CREATE|ALTER|DROP).*(?:FROM|INTO|TABLE|DATABASE)', 'SQL Query'),
        ]
        
        for pattern, pattern_name in sensitive_patterns:
            matches = re.findall(pattern, content)
            if matches:
                # Get context for each match (without exposing the actual sensitive data)
                contexts = []
                for match in matches[:3]:  # Limit to first 3 matches
                    sanitized_match = '[REDACTED]'
                    try:
                        # Find the match in the content and get surrounding context
                        match_pos = content.find(match)
                        if match_pos != -1:
                            start_pos = max(0, match_pos - 40)
                            end_pos = min(len(content), match_pos + len(match) + 40)
                            context = content[start_pos:match_pos] + sanitized_match + content[match_pos+len(match):end_pos]
                            contexts.append(context)
                    except:
                        pass
                
                finding_description = f'Found potential sensitive information in the page content matching pattern: {pattern_name}'
                findings.append({
                    'name': 'Information Disclosure',
                    'description': finding_description,
                    'severity': 'medium',
                    'details': {
                        'page_url': page_url,
                        'pattern_type': pattern_name,
                        'count': len(matches),
                        'context_examples': contexts,
                        'impact': 'Sensitive information can be leaked to unauthorized users and potentially misused.',
                        'recommendation': 'Remove sensitive information from public-facing pages and review code for information leakage.'
                    }
                })
        
        return findings
    
    def _check_javascript_libraries(self, soup, page_url):
        findings = []
        
        # Check for outdated or vulnerable JavaScript libraries
        scripts = soup.find_all('script', src=True)
        for script in scripts:
            src = script['src']
            lib_name = None
            version = None
            is_outdated = False
            
            # Check for common JavaScript libraries
            library_patterns = [
                (r'jquery[.-](\d+\.\d+\.\d+)', 'jQuery', lambda v: v.startswith('1.') or v.startswith('2.')),
                (r'angular[.-](\d+\.\d+\.\d+)', 'Angular', lambda v: v.startswith('1.')),
                (r'bootstrap[.-](\d+\.\d+\.\d+)', 'Bootstrap', lambda v: v.startswith('2.') or v.startswith('3.')),
                (r'react[.-](\d+\.\d+\.\d+)', 'React', lambda v: v < '16.0.0'),
                (r'vue[.-](\d+\.\d+\.\d+)', 'Vue.js', lambda v: v.startswith('1.')),
                (r'moment[.-](\d+\.\d+\.\d+)', 'Moment.js', lambda v: v < '2.29.0'),
                (r'lodash[.-](\d+\.\d+\.\d+)', 'Lodash', lambda v: v < '4.17.20'),
            ]
            
            for pattern, name, version_check in library_patterns:
                match = re.search(pattern, src)
                if match:
                    lib_name = name
                    version = match.group(1)
                    is_outdated = version_check(version)
                    break
            
            if lib_name and is_outdated:
                findings.append({
                    'name': f'Outdated {lib_name} Version',
                    'description': f'The website is using an outdated version of {lib_name}: {version}',
                    'severity': 'medium',
                    'details': {
                        'page_url': page_url,
                        'library': lib_name,
                        'version': version,
                        'file': src,
                        'script_tag': str(script),
                        'impact': 'Outdated JavaScript libraries may contain known security vulnerabilities that can be exploited.',
                        'recommendation': f'Update to the latest version of {lib_name} to protect against known vulnerabilities.'
                    }
                })
                
            # Also check for libraries loaded from CDNs without integrity checks
            if ('cdn' in src or '//' in src) and not script.get('integrity'):
                if lib_name:
                    lib_description = f"{lib_name} ({version})"
                else:
                    lib_description = src.split('/')[-1]
                
                findings.append({
                    'name': 'Missing Subresource Integrity',
                    'description': f'A JavaScript library loaded from external source has no integrity check: {lib_description}',
                    'severity': 'low',
                    'details': {
                        'page_url': page_url,
                        'script_src': src,
                        'script_tag': str(script),
                        'impact': 'Without integrity checks, the content of external scripts could be modified by an attacker without detection.',
                        'recommendation': 'Add integrity attributes to script tags for externally hosted libraries.'
                    }
                })
        
        return findings