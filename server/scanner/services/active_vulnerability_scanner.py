# backend/scanner/services/active_vulnerability_scanner.py - ENHANCED VERSION with explicit test path payloads and login SQLi

import requests
import logging
import re
import time
import socket
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin, parse_qs, urlunparse, urlencode
from django.utils import timezone

logger = logging.getLogger(__name__)

class ActiveVulnerabilityScanner:
    def __init__(self, url, user=None, compliance_mode='strict', skip_rate_limiting=False):
        self.url = url
        self.user = user
        self.base_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}"
        self.domain = urlparse(url).netloc
        self.compliance_mode = compliance_mode
        self.skip_rate_limiting = skip_rate_limiting

        self.headers = {
            'User-Agent': f'Site-Analyser Active Scanner/1.0 (Compliance Mode: {compliance_mode})'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)

        self.scanned_urls = set()
        self.scan_start_time = time.time()
        self.requests_made = 0
        self.last_request_time = 0

    def scan(self):
        findings = []
        pages_to_scan = [self.url]

        for page_url in pages_to_scan:
            if page_url in self.scanned_urls:
                continue

            self.scanned_urls.add(page_url)
            resp = self._make_request('GET', page_url)
            if not resp:
                continue

            soup = BeautifulSoup(resp.text, 'html.parser')
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(self.base_url, href)
                if self._is_scannable_url(full_url):
                    pages_to_scan.append(full_url)

            if self.compliance_mode in ['moderate', 'permissive']:
                findings.extend(self._test_sql_injection())
                findings.extend(self._test_xss_vulnerabilities())
                findings.extend(self._test_known_vulnerable_paths())
                findings.extend(self._test_login_form_sql_injection())

        return findings

    def _make_request(self, method, url, **kwargs):
        try:
            time.sleep(1.0)
            self.requests_made += 1
            return self.session.request(method, url, timeout=10, **kwargs)
        except requests.RequestException as e:
            logger.error(f"Request failed for {url}: {e}")
            return None

    def _extract_form_data_safe(self, form):
        form_data = {}
        inputs = form.find_all(['input', 'textarea', 'select'])[:5]
        for input_field in inputs:
            name = input_field.get('name')
            if not name:
                continue
            form_data[name] = 'test'
        return form_data

    def _test_known_vulnerable_paths(self):
        paths = [
            '/search.jsp?q=<script>alert(1)</script>',
            '/login.jsp',
            '/bank/login.aspx'
        ]
        findings = []
        for path in paths:
            url = urljoin(self.base_url, path)
            resp = self._make_request('GET', url)
            if not resp:
                continue
            if '<script>alert(1)</script>' in resp.text:
                findings.append({
                    'name': 'Reflected XSS in known path',
                    'description': f'Reflected XSS payload echoed in {path}',
                    'severity': 'high',
                    'details': {'url': url}
                })
            elif any(err in resp.text.lower() for err in ['sql', 'exception', 'error']):
                findings.append({
                    'name': 'Known Test Path Clue',
                    'description': f'Path {path} shows error-like content',
                    'severity': 'medium',
                    'details': {'url': url}
                })
        return findings

    def _test_login_form_sql_injection(self):
        login_url = urljoin(self.base_url, '/login.jsp')
        payloads = ["' OR '1'='1", "admin'--"]
        findings = []

        for payload in payloads:
            data = {
                'uid': payload,
                'passw': 'anyvalue',
                'btnSubmit': 'Login'
            }
            resp = self._make_request('POST', login_url, data=data)
            if resp and ('Welcome' in resp.text or 'Logout' in resp.text):
                findings.append({
                    'name': 'SQL Injection - Login Bypass',
                    'description': 'Possible login bypass using SQL injection payload',
                    'severity': 'high',
                    'details': {
                        'url': login_url,
                        'payload': payload
                    }
                })
        return findings

    def _test_sql_injection(self):
        payloads = ["' OR '1'='2", "admin'--", "test' OR 1=2 --"]
        findings = []

        for page_url in list(self.scanned_urls)[:3]:
            resp = self._make_request('GET', page_url)
            if not resp:
                continue

            soup = BeautifulSoup(resp.text, 'html.parser')
            forms = soup.find_all('form')[:2]
            for form in forms:
                form_data = self._extract_form_data_safe(form)
                method = form.get('method', 'GET').upper()
                action = form.get('action', '')
                form_url = urljoin(page_url, action)

                clean_response = self._make_request(method, form_url, data=form_data if method == 'POST' else None, params=form_data if method == 'GET' else None)

                for field in list(form_data.keys())[:1]:
                    for payload in payloads:
                        test_data = form_data.copy()
                        test_data[field] = payload
                        response = self._make_request(method, form_url, data=test_data if method == 'POST' else None, params=test_data if method == 'GET' else None)
                        if response and self._check_sql_errors_safe(clean_response.text, response.text):
                            findings.append({
                                'name': 'Potential SQL Injection',
                                'description': f'SQLi suspected in field {field}',
                                'severity': 'high',
                                'details': {
                                    'field': field,
                                    'url': form_url,
                                    'payload': '[REDACTED]',
                                    'method': method
                                }
                            })
                            break
        return findings

    def _check_sql_errors_safe(self, clean_text, test_text):
        return abs(len(clean_text) - len(test_text)) > 100 or any(
            err in test_text.lower() for err in ['sql syntax error', 'mysql error', 'invalid query', 'sqlite error'])

    def _test_xss_vulnerabilities(self):
        payloads = ['<test>', '\"><test>', "'><test>"]
        findings = []

        for page_url in list(self.scanned_urls)[:2]:
            resp = self._make_request('GET', page_url)
            if not resp:
                continue
            soup = BeautifulSoup(resp.text, 'html.parser')

            parsed = urlparse(page_url)
            if parsed.query:
                params = parse_qs(parsed.query)
                for key in list(params.keys())[:1]:
                    for payload in payloads:
                        test_params = params.copy()
                        test_params[key] = [payload]
                        new_query = urlencode(test_params, doseq=True)
                        test_url = urlunparse((parsed.scheme, parsed.netloc, parsed.path, parsed.params, new_query, parsed.fragment))
                        response = self._make_request('GET', test_url)
                        if response and payload in response.text:
                            context = response.text[response.text.find(payload)-30:response.text.find(payload)+30].lower()
                            if '<script' in context or 'onerror' in context or 'onclick' in context:
                                findings.append({
                                    'name': 'Reflected XSS',
                                    'description': f'Possible XSS in parameter {key}',
                                    'severity': 'high',
                                    'details': {
                                        'parameter': key,
                                        'url': test_url,
                                        'payload': '[SAFE_PAYLOAD]'
                                    }
                                })

            forms = soup.find_all('form')[:2]
            for form in forms:
                form_data = self._extract_form_data_safe(form)
                method = form.get('method', 'GET').upper()
                action = form.get('action', '')
                form_url = urljoin(page_url, action)
                for field in list(form_data.keys())[:1]:
                    for payload in payloads:
                        test_data = form_data.copy()
                        test_data[field] = payload
                        resp = self._make_request(method, form_url, data=test_data if method == 'POST' else None, params=test_data if method == 'GET' else None)
                        if resp and payload in resp.text:
                            context = resp.text[resp.text.find(payload)-30:resp.text.find(payload)+30].lower()
                            if '<script' in context or 'onerror' in context or 'onclick' in context:
                                findings.append({
                                    'name': 'Form Reflected XSS',
                                    'description': f'Reflected XSS in form field {field}',
                                    'severity': 'high',
                                    'details': {
                                        'field': field,
                                        'url': form_url,
                                        'payload': '[SAFE_PAYLOAD]'
                                    }
                                })
        return findings

    def _is_scannable_url(self, url):
        if urlparse(url).netloc != self.domain:
            return False
        skip_exts = ['.jpg', '.jpeg', '.png', '.gif', '.css', '.js', '.pdf']
        return not any(url.lower().endswith(ext) for ext in skip_exts)
